{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"evaluate.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTQdkGw6-lVl","executionInfo":{"status":"ok","timestamp":1634607569576,"user_tz":-480,"elapsed":22188,"user":{"displayName":"Yi-Jing Sie","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08533209739136415786"}},"outputId":"87a24892-b6d2-46de-b643-8fbc65cfd392"},"source":["\n","PATH = 'VRDL/VRDL_HW1_310554010' #you should key in the path of folder that contain this ipynb file\n","'''\n","for example, in my setting, I put this ipynb file folder in 'VRDL/VRDL_HW1_310554010' on My Drive\n","then PATH will be\n","'VRDL/VRDL_HW1_310554010'\n","'''\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","!ls gdrive/MyDrive/\n","!nvidia-smi\n","\n","import os\n","os.chdir('./gdrive/MyDrive/')\n","\n","os.chdir(f'./{PATH}')"],"id":"MTQdkGw6-lVl","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"," data\t        DLP_final  'Meet Recordings'   tutorial.ipynb   workshop.pdf\n","'data mining'   hw3.ipynb   review_paper       VRDL\n","Tue Oct 19 01:39:29 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"h-m61udNLWcC"},"source":[""],"id":"h-m61udNLWcC","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3266b08","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634607588584,"user_tz":-480,"elapsed":15576,"user":{"displayName":"Yi-Jing Sie","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08533209739136415786"}},"outputId":"16e11e6e-43c8-4636-fac9-e9c80d597a72"},"source":["import random\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import birdloader\n","from birdloader import BirdImage as BirdImage\n","import function as fn"],"id":"d3266b08","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f26955c7370>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"U2lgE4FPcM-v"},"source":["'''\n","this procedure will generate an answer.txt in your current folder, \n","please back to your current folder, and check answer.txt \n","'''\n","#prepare data\n","testset = BirdImage('test')\n","test_loader = DataLoader(dataset = testset,\n","            batch_size = 16,\n","            shuffle = False,\n","            num_workers = 0,\n","            pin_memory = True)\n","#load data\n","model1 = models.resnet152(pretrained=False)\n","model1.fc = nn.Linear(2048, 200)\n","\n","model2 = models.resnet152(pretrained=False)\n","model2.fc = nn.Linear(2048, 200)\n","\n","model3 = models.resnet152(pretrained=False)\n","model3.fc = nn.Linear(2048, 200)\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cuda:0\")\n","model1.to(device)\n","model1.eval()\n","model1.load_state_dict(torch.load('./model-para/ep8 acc1.00 0.65.pt', map_location=\"cuda:0\"))\n","\n","model2.to(device)\n","model2.eval()\n","model2.load_state_dict(torch.load('./model-para/ep30 acc0.99 0.65.pt', map_location=\"cuda:0\"))\n","\n","model3.to(device)\n","model3.eval()\n","model3.load_state_dict(torch.load('./model-para/ep8_finetune.pt', map_location=\"cuda:0\"))\n","\n","\n","#start evaluating\n","final = torch.empty(0).to(device)\n","for x in tqdm(test_loader):\n","  x = x.to(device).float()\n","  out1 = model1(x)\n","  out2 = model2(x)\n","  out3 = model3(x)\n","  out = out1 + out2 + out3\n","  out = torch.max(out, 1).indices.view(-1)\n","  final = torch.cat([final, out], dim=0)\n","  \n","#convert index to bird name\n","final = fn.ind2class(final)\n","#write final to answer.txt\n","fn.class2txt(final)\n","\n"," "],"id":"U2lgE4FPcM-v","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5qgC6OZNRmW"},"source":[""],"id":"_5qgC6OZNRmW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qm0RCKsZWNR9"},"source":[""],"id":"qm0RCKsZWNR9","execution_count":null,"outputs":[]}]}